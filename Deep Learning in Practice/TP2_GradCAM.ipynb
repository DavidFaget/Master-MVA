{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st3i8hrJCqZn"
   },
   "source": [
    "\n",
    "<font color=\"blue\"> <strong>Students:</strong> Biel CASTAÑO and David FAGET\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "id": "NVqeNcHkJzxy"
   },
   "source": [
    "## Visualization of CNN: Grad-CAM\n",
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n",
    "\n",
    "\n",
    "* NB: if `PIL` is not installed, try `conda install pillow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "id": "QWbb_Wx3Jzx0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "id": "z689UF0xJzx1"
   },
   "source": [
    "### Download the Model\n",
    "We provide you a pretrained model `ResNet-34` for `ImageNet` classification dataset.\n",
    "* **ImageNet**: A large dataset of photographs with 1 000 classes.\n",
    "* **ResNet-34**: A deep architecture for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHY0VoW9Jzx1",
    "outputId": "f3233934-b161-4512-bcc9-57545fa0668b"
   },
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(weights='ResNet34_Weights.IMAGENET1K_V1')  # New PyTorch interface for loading weights!\n",
    "resnet34.eval() # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "id": "o1JwyUWsJzx2"
   },
   "source": [
    "![ResNet34](https://miro.medium.com/max/1050/1*Y-u7dH4WC-dXyn9jOG4w0w.png)\n",
    "\n",
    "\n",
    "Input image must be of size (3x224x224).\n",
    "\n",
    "First convolution layer with maxpool.\n",
    "Then 4 ResNet blocks.\n",
    "\n",
    "Output of the last ResNet block is of size (512x7x7).\n",
    "\n",
    "Average pooling is applied to this layer to have a 1D array of 512 features fed to a linear layer that outputs 1000 values (one for each class). No softmax is present in this case. We have already the raw class score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYuXKplRJzx2",
    "outputId": "03620d51-8699-4090-effd-efa770366167"
   },
   "outputs": [],
   "source": [
    "classes = pickle.load(urllib.request.urlopen('https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'))\n",
    "\n",
    "##classes is a dictionary with the name of each class\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "id": "dlzupqzmJzx2"
   },
   "source": [
    "### Input Images\n",
    "We provide you 20 images from ImageNet (download link on the webpage of the course or download directly using the following command line,).<br>\n",
    "In order to use the pretrained model resnet34, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "id": "65OTE2v5Jzx2"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(dir_path):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    # Note: If the inverse normalisation is required, apply 1/x to the above object\n",
    "\n",
    "    dataset = datasets.ImageFolder(dir_path, transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224), # resize the image to 224x224\n",
    "            transforms.ToTensor(), # convert numpy.array to tensor\n",
    "            normalize])) #normalize the tensor\n",
    "\n",
    "    return (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1JOh3oCJzx3",
    "outputId": "c278ab63-47ae-483e-f112-c4a3b46629f6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "if not os.path.exists(\"data/TP2_images\"):\n",
    "    os.mkdir(\"data/TP2_images\")\n",
    "    !cd data/TP2_images && wget \"https://www.lri.fr/~gcharpia/deeppractice/2023/TP2/TP2_images.zip\" && unzip TP2_images.zip\n",
    "\n",
    "dir_path = \"data/\"\n",
    "dataset = preprocess_image(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "o-2ty4iDJzx3",
    "outputId": "aeb1a5d1-cabe-4154-eb76-e7707bca5edf"
   },
   "outputs": [],
   "source": [
    "# show the orignal image\n",
    "index = 5\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
    "plt.imshow(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CEulDnInJzx3",
    "outputId": "a930e30f-85ac-4dad-b1f5-8047688bf3a1"
   },
   "outputs": [],
   "source": [
    "output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n",
    "values, indices = torch.topk(output, 3)\n",
    "print(\"Top 3-classes:\", indices[0].numpy(), [classes[x] for x in indices[0].numpy()])\n",
    "print(\"Raw class scores:\", values[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "id": "dzAq8SUtJzx4"
   },
   "source": [
    "### Grad-CAM\n",
    "* **Overview:** Given an image, and a category (‘tiger cat’) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
    "\n",
    "\n",
    "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the given images. For each image, choose the top-3 possible labels as the desired classes. Compare the heatmaps of the three classes, and conclude.\n",
    "\n",
    "\n",
    "* **To be submitted within 2 weeks**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
    "\n",
    "\n",
    "* **Hints**:\n",
    " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully.\n",
    " + More on [autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) and [hooks](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks)\n",
    " + The pretrained model resnet34 doesn't have an activation function after its last layer, the output is indeed the `raw class scores`, you can use them directly.\n",
    " + The size of feature maps is 7x7, so your heatmap will have the same size. You need to project the heatmap to the resized image (224x224, not the original one, before the normalization) to have a better observation. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  \n",
    " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "id": "-xFsOMZwJzx4"
   },
   "source": [
    "Class: ‘pug, pug-dog’ | Class: ‘tabby, tabby cat’\n",
    "- | -\n",
    "![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog.jpg)| ![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/cat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKNFFjWMBCqu"
   },
   "outputs": [],
   "source": [
    "# First, let's declare a class to handle the Grad-CAM algorithm\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        self.features = None\n",
    "\n",
    "        # Register hooks\n",
    "        self.hook_handlers = [\n",
    "          target_layer.register_forward_hook(self.save_features),\n",
    "          target_layer.register_backward_hook(self.save_gradients),\n",
    "        ]\n",
    "\n",
    "    def save_features(self, module, input, output):\n",
    "        self.features = output.detach()\n",
    "\n",
    "    def save_gradients(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        # Forward pass\n",
    "        output = self.model(image)\n",
    "\n",
    "        # Set output for backpropagation\n",
    "        one_hot_output = F.one_hot(torch.tensor([label]), num_classes=output.size(-1))\n",
    "        one_hot_output = one_hot_output.to(dtype=torch.float).requires_grad_(True)\n",
    "        one_hot_output = torch.sum(one_hot_output * output)\n",
    "\n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        one_hot_output.backward()\n",
    "\n",
    "        # Gradients and features to numpy\n",
    "        gradients = self.gradients.numpy().squeeze(0)\n",
    "        features = self.features.numpy().squeeze(0)\n",
    "\n",
    "        # Compute filter weights\n",
    "        filter_weights = np.mean(gradients, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
    "\n",
    "        # Compute weighted feature map\n",
    "        heatmap = np.sum(filter_weights * features, axis=0)\n",
    "\n",
    "        # ReLU on top of the heatmap\n",
    "        heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "        # Normalize the heatmap\n",
    "        heatmap /= np.max(heatmap)\n",
    "\n",
    "        # Resize heatmap\n",
    "        heatmap = torch.from_numpy(heatmap).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimension\n",
    "        heatmap = F.interpolate(heatmap, size=image.size()[2:], mode='bilinear')\n",
    "        heatmap = heatmap.numpy().squeeze(0).squeeze(0)  # remove batch and channel dimension\n",
    "\n",
    "        self.remove_hooks() # Need to free memory\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for handler in self.hook_handlers:\n",
    "            handler.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1y4O7ZCdTg4Y"
   },
   "outputs": [],
   "source": [
    "# Let's now define a function to plot the grid of heatmaps for the selected layer\n",
    "# containing the 3 top classes for each image on the dataset\n",
    "\n",
    "def plot_grad_cam(dataset, model, selected_layer, classes):\n",
    "    for i in range(len(dataset)):\n",
    "        # Load input\n",
    "        img_path, _ = dataset.imgs[i]\n",
    "        img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
    "        input = dataset[i][0].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Get the 3 top classes via forward pass\n",
    "        output = model(input)\n",
    "        values, indices = torch.topk(output, 3)\n",
    "\n",
    "        # Define image grid\n",
    "        f, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "        # Plot input image\n",
    "        ax[0].imshow(img)\n",
    "        ax[0].set_title(f'Test image nº {i+1}')\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        for j in range(1, 4):\n",
    "            label = indices[0][j-1].item()\n",
    "\n",
    "            # Compute heatmap\n",
    "            grad_cam = GradCAM(model, selected_layer)\n",
    "            heatmap = grad_cam(input, label)\n",
    "\n",
    "            # Apply colormap to heatmap\n",
    "            heat = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "            heat = np.float32(heat) / 255\n",
    "\n",
    "            # Merge heatmap with original image\n",
    "            merged_image = cv2.addWeighted(heat, 0.5, img, 0.5, 0)\n",
    "            merged_image = np.uint8(255 * merged_image[:, :, ::-1])  # Convert BGR to RGB\n",
    "\n",
    "            # Plot heatmap\n",
    "            ax[j].imshow(merged_image)\n",
    "            ax[j].axis('off')\n",
    "            name = classes[label].split(',')[0] if classes else f'Class {label}'\n",
    "            ax[j].set_title(name)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SBzRuKpFWhQl",
    "outputId": "80f5b78f-744e-46aa-e308-bd3cfd1ea92b"
   },
   "outputs": [],
   "source": [
    "# Apply Grad-CAM to get last layer's heatmaps (after bn2).\n",
    "\n",
    "plot_grad_cam(dataset, resnet34, resnet34.layer4[2].bn2, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJHQvaujQCuf"
   },
   "source": [
    "<font color=\"blue\"> Comments:\n",
    "\n",
    "<font color=\"blue\"> Frequently, it's observed that the heatmaps converge on the same regions across the three labels, which is explained by the visual similarity of the 3 animals predicted. However, there are some important and interesting exceptions. We are going to comment the most curious ones:\n",
    "\n",
    " - <font color=\"blue\"> For test image nº8, we observe that the heatmap corresponding to \"chesapeake bay retriever\" focuses much more on the dog that is behind.\n",
    "\n",
    " - <font color=\"blue\"> For test image nº11, we observe that only the first prediction is correct. The two last ones are false. Indeed, the network correctly predicts a horse when it focuses on the entire body of the animal. When it focuses only on the neck, it gives incorrect predictions such as ox or basenji.\n",
    "\n",
    " - <font color=\"blue\"> Test image nº13 is also curious. We observe that the network fails completely to detect the animal, which is explained by the fact that it focuses more on the borders than on the animals.\n",
    "\n",
    "\n",
    " - <font color=\"blue\"> For test image nº16, the network is only right for its first prediction (when it sees all the animal). We observe that when it predicts a cowboy boot or a balance beam (unrelated objects), the network only sees a small portion of the body.\n",
    "\n",
    "\n",
    "<font color=\"blue\"> In summary, GradCAM offers valuable insights into the inner workings of the network and its perception during prediction. It aids in comprehending the variations in predictions and their association with particular regions identified by the network within the input image.\n",
    "\n",
    "<font color=\"blue\"> But, what would the outcome be if we applied Grad-CAM to layers other than the final one? We will answer this question in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "id": "vWAcUkjsJzx4"
   },
   "source": [
    "### Complementary questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "id": "2Ww393PlJzx5"
   },
   "source": [
    "##### Try GradCAM on others convolutional layers, describe and comment the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxttOqYeuLFR"
   },
   "source": [
    "<font color=\"blue\"> We will write layer{i}[j] to refer to the BasicBlock nºj of the layer nºi (see the architecture above). We will always compute the heatmap after bn2 (batch normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LHTa6I4Jb7jt",
    "outputId": "881228e4-d813-4d0a-c502-b8e13852de6d"
   },
   "outputs": [],
   "source": [
    "# Apply Grad-CAM to get the layer{4}[1] heatmap (this is the second last BasicBlock; heatmaps for the last one were obtained above)\n",
    "\n",
    "plot_grad_cam(dataset, resnet34, resnet34.layer4[1].bn2, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Rb79YfnIXr-z",
    "outputId": "acf00752-2470-429c-deff-b874d1f34e33"
   },
   "outputs": [],
   "source": [
    "# Layer{3}[5]\n",
    "\n",
    "plot_grad_cam(dataset, resnet34, resnet34.layer3[5].bn2, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "H4VtMUBBX54L",
    "outputId": "e324d720-1f0a-4c8a-b67b-37c746ba5a84"
   },
   "outputs": [],
   "source": [
    "# Layer{2}[3]\n",
    "\n",
    "plot_grad_cam(dataset, resnet34, resnet34.layer2[3].bn2, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kPGUNYS6P7_"
   },
   "source": [
    "<font color=\"blue\"> Comments:\n",
    "\n",
    "- <font color=\"blue\"> For layer{4}[1], we observe that heatmaps are close to what we obtained for the last layer. This is coherent with what we expected to have. However, we also observe some differences. For example, for test image nº8, the first prediction focuses more on the dog that is behind, while the last layer focused more on the dog at the front. Other examples such as the one of the horse (test image nº11) are also interesting. Here, we observe that when the models predict an ox, layer{4}[1] focuses more on the queue than the last layer.\n",
    "\n",
    "- <font color=\"blue\"> For layer{3}[5], we generally do not observe informative heatmaps. However, there are some exceptions. For example, we see that for test image nº10 all three heatmaps focus on the horns, and that for test image nº16 (sea lion), the last heatmap focuses on the head.\n",
    "\n",
    "\n",
    "- <font color=\"blue\"> In the case of layer{2}[3], it is the same: heatmaps are generally not informative. However, we observe some details. For example, in test image nº7 (fox), heatmaps focus on the eyes and nose, while in test image nº10, heatmaps focus on the feet.\n",
    "\n",
    "<font color=\"blue\"> In conclusion, as we move away from the final layer towards the initial layers, the heatmaps become less informative for human interpretability. This is expected, since CNN represent a hierarchical structure, with early layers capturing low-level features across the image, such as edges or local patterns. Progressing deeper into the network, layers begin to detect more complex patterns, including parts of objects like ears or eyes. And finally, the final leyers of a CNN are highly specialized for the classification task, so the heatmaps from these layers highlight regions of the image that are important for making the classification decision. This explains why it is difficult for humans to interpret the first layers (for example, layer{2}[3], that offers limited information for human interpretability), whereas it is less challenging to interpret middle layers (like layer layer{3}[5], where we start to see some parts of the objects that have been important for the decision, like horns in test image 10), and final layers are completely interpretable (as seen in layer 4 examples, where the parts of the image that were relevant for the classification align with human decisions).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "9k2pxEkJNCY9",
    "id": "QoLMlqVTJzx5"
   },
   "source": [
    "##### What are the principal contributions of GradCAM (the answer is in the paper) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d74-M_Ud9kMD"
   },
   "source": [
    "<font color=\"blue\"> We will <font color=\"green\"> quote <font color=\"blue\"> the authors' contributions verbatim, and then explain and summarise them:\n",
    "\n",
    "<font color=\"green\"> (1) We introduce Grad-CAM, a class-discriminative localization technique that generates visual explanations for any\n",
    "CNN-based network without requiring architectural changes\n",
    "or re-training. We evaluate Grad-CAM for localization (Sec. 4.1),\n",
    "and faithfulness to model (Sec. 5.3), where it outperforms\n",
    "baselines.\n",
    "\n",
    "<font color=\"green\"> (2) We apply Grad-CAM to existing top-performing classification, captioning (Sec. 8.1), and VQA (Sec. 8.2) models.\n",
    "For image classification, our visualizations lend insight into\n",
    "failures of current CNNs (Sec. 6.1), showing that seemingly\n",
    "unreasonable predictions have reasonable explanations. For\n",
    "captioning and VQA, our visualizations expose that common\n",
    "CNN + LSTM models are often surprisingly good at localizing discriminative image regions despite not being trained\n",
    "on grounded image-text pairs.\n",
    "\n",
    "<font color=\"green\"> (3) We show a proof-of-concept of how interpretable GradCAM visualizations help in diagnosing failure modes by\n",
    "uncovering biases in datasets. This is important not just for\n",
    "generalization, but also for fair and bias-free outcomes as\n",
    "more and more decisions are made by algorithms in society.\n",
    "\n",
    "<font color=\"green\"> (4) We present Grad-CAM visualizations for ResNets [24]\n",
    "applied to image classification and VQA (Sec. 8.2).\n",
    "\n",
    "<font color=\"green\"> (5) We use neuron importance from Grad-CAM and neuron\n",
    "names from [4] and obtain textual explanations for model\n",
    "decisions (Sec. 7).\n",
    "\n",
    "<font color=\"green\"> (6) We conduct human studies (Sec. 5) that show Guided\n",
    "Grad-CAM explanations are class-discriminative and not\n",
    "only help humans establish trust, but also help untrained users\n",
    "successfully discern a ‘stronger’ network from a ‘weaker’\n",
    "one, even when both make identical predictions.\n",
    "\n",
    "\n",
    "<font color=\"blue\">\n",
    "In summary, Grad-CAM is a visualization technique that enhances the interpretability of CNN-based models without requiring any architectural modifications or retraining. It is effective in various domains, including image classification, captioning, and visual question answering (VQA), by providing insights into model decisions. Key contributions of Grad-CAM include its ability to localize relevant image regions, diagnose dataset biases, and offer a diagnostic tool for improving model fairness and generalization. Additionally, Grad-CAM facilitates the generation of textual explanations for model decisions and has been validated through human studies to improve trust in AI systems and helps in distinguishing between models of varying reliability."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kfiletag": "9k2pxEkJNCY9",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
