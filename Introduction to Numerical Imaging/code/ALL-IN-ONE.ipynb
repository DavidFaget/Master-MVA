{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61375eb9",
   "metadata": {},
   "source": [
    "# Code ALL-IN-ONE for the Project IIN MVA 2023/2024, David and Gabriel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e824187a",
   "metadata": {},
   "source": [
    "The goal is to use already trained models found at https://github.com/VITA-Group/All-In-One-Underwater-Image-Enhancement-using-Domain-Adversarial-Learning/tree/master for our own images. This was very time-consuming for us since we had to change several parts of the code, but we finally managed to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1dcb8",
   "metadata": {},
   "source": [
    "## Library importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a91de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ade10c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim_fn\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr_fn\n",
    "from collections import defaultdict\n",
    "import click\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeab65f",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419c255",
   "metadata": {},
   "source": [
    "Here, we had to change several things from the original code. First, we had to change size and test_start values. Then, we had to add the loading of clear images (cl_img)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3aff6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class UIEBDataset(Dataset):\n",
    "    def __init__(self, data_path, label_path, img_format='png', size=10, mode='test', train_start=0, val_start=30000, test_start=500):\n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        self.mode = mode\n",
    "        self.size = size\n",
    "        self.train_start = train_start\n",
    "        self.test_start = test_start\n",
    "        self.val_start = val_start\n",
    "\n",
    "        self.uw_images = glob(os.path.join(self.data_path, '*.' + img_format))\n",
    "        print(\"Found uw images:\", len(self.uw_images))\n",
    "        \n",
    "        self.cl_images = glob(os.path.join(self.label_path, '*.' + img_format))\n",
    "        print(\"Found cl images:\", len(self.uw_images))\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self.uw_images = self.uw_images[self.train_start:self.train_start+self.size]\n",
    "        elif self.mode == 'test':\n",
    "            self.uw_images = self.uw_images[self.test_start:self.test_start+self.size]\n",
    "            self.cl_images = self.cl_images[self.test_start:self.test_start+self.size]\n",
    "        elif self.mode == 'val':\n",
    "            self.uw_images = self.uw_images[self.val_start:self.val_start+self.size]\n",
    "        \n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index >= len(self.uw_images):\n",
    "            raise IndexError(f\"Index {index} is out of bounds for dataset with size {len(self.uw_images)}\")\n",
    "\n",
    "        uw_img = self.transform(Image.open(self.uw_images[index]))\n",
    "        cl_img = self.transform(Image.open(self.cl_images[index]))\n",
    "        return uw_img, cl_img, -1, os.path.basename(self.uw_images[index]) #The last is the name\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046d8a8",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "edd6000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UNetEncoder(nn.Module):\n",
    "    def __init__(self, n_channels=3):\n",
    "        super(UNetEncoder, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        return x5, (x1, x2, x3, x4)\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, n_channels=3):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        self.up1 = up(1024, 256)\n",
    "        self.up2 = up(512, 128)\n",
    "        self.up3 = up(256, 64)\n",
    "        self.up4 = up(128, 64)\n",
    "        self.outc = outconv(64, n_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, enc_outs):\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.up1(x, enc_outs[3])\n",
    "        x = self.up2(x, enc_outs[2])\n",
    "        x = self.up3(x, enc_outs[1])\n",
    "        x = self.up4(x, enc_outs[0])\n",
    "        x = self.outc(x)\n",
    "        return nn.Tanh()(x)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            Flatten(),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, num_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.classifier(input)\n",
    "    \n",
    "    \n",
    "    \n",
    "# UNET PARTS:\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        \n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2eb2de",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5193232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    return x\n",
    "\n",
    "def var_to_img(img):\n",
    "    return (img * 255).cpu().data.numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "\n",
    "def test(fE, fI, dataloader, model_name, which_epoch):\n",
    "    mse_scores = []\n",
    "    ssim_scores = []\n",
    "    psnr_scores = []\n",
    "    criterion_MSE = nn.MSELoss().cuda()\n",
    "\n",
    "    for idx, data in tqdm(enumerate(dataloader)):\n",
    "        uw_img, cl_img, water_type, name = data\n",
    "        uw_img = Variable(uw_img)\n",
    "        cl_img = Variable(cl_img, requires_grad=False)\n",
    "        \n",
    "        fE_out, enc_outs = fE(uw_img)\n",
    "        fI_out = to_img(fI(fE_out, enc_outs).detach())\n",
    "        enc_outs = None\n",
    "        \n",
    "        print(\"uw_img shape:\", uw_img.squeeze().cpu().data.shape)\n",
    "        print(\"fI_out shape:\", fI_out.squeeze().cpu().data.shape)\n",
    "        print(\"cl_img shape:\", cl_img.squeeze().cpu().data.shape)\n",
    "\n",
    "        save_image(torch.stack([uw_img.squeeze().cpu().data, fI_out.squeeze().cpu().data, cl_img.squeeze().cpu().data]), 'C:/Users/davfa/Desktop/results/{}/{}/{}_{}.jpg'.format(model_name, which_epoch, name[0], 'out'))\n",
    "\n",
    "        mse = criterion_MSE(fI_out, cl_img).item()\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "        fI_out = (fI_out * 255).squeeze(0).cpu().data.numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "        cl_img = (cl_img * 255).squeeze(0).cpu().data.numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "\n",
    "       # ssim = ssim_fn(fI_out, cl_img, multichannel=True)\n",
    "       # psnr = psnr_fn(cl_img, fI_out)\n",
    "\n",
    "       # ssim_scores.append(ssim)\n",
    "        #psnr_scores.append(psnr)\n",
    "\n",
    "    return _,_,mse_scores #ssim_scores, psnr_scores, mse_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e84e671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(name, num_channels, test_dataset, data_path, label_path, which_epoch, test_size, fe_load_path, fi_load_path):\n",
    "\n",
    "    if not os.path.exists('C:/Users/davfa/Desktop/results'):\n",
    "        os.mkdir('C:/Users/davfa/Desktop/results')\n",
    "\n",
    "    if not os.path.exists('C:/Users/davfa/Desktop/results/{}'.format(name)):\n",
    "        os.mkdir('C:/Users/davfa/Desktop/results/{}'.format(name))\n",
    "\n",
    "    if not os.path.exists('C:/Users/davfa/Desktop/results/{}/{}'.format(name, which_epoch)):\n",
    "        os.mkdir('C:/Users/davfa/Desktop/results/{}/{}'.format(name, which_epoch))\n",
    "\n",
    "    fE_load_path = fe_load_path\n",
    "    fI_load_path = fi_load_path\n",
    "\n",
    "    fE = UNetEncoder(num_channels)\n",
    "    fI = UNetDecoder(num_channels)\n",
    "\n",
    "    if which_epoch:\n",
    "        fE.load_state_dict(torch.load(os.path.join('C:/Users/davfa/Downloads', name, 'fE_{}.pth'.format(which_epoch)),map_location=torch.device('cpu')))\n",
    "        fI.load_state_dict(torch.load(os.path.join('C:/Users/davfa/Downloads', name, 'fI_{}.pth'.format(which_epoch)),map_location=torch.device('cpu')))\n",
    "    else:\n",
    "        fE.load_state_dict(torch.load(fE_load_path,map_location=torch.device('cpu')))\n",
    "        fI.load_state_dict(torch.load(fI_load_path,map_location=torch.device('cpu')))\n",
    "\n",
    "    fE.eval()\n",
    "    fI.eval()\n",
    "\n",
    "    if test_dataset=='nyu':\n",
    "        test_dataset = NYUUWDataset(data_path, \n",
    "            label_path,\n",
    "            size=3000,\n",
    "            test_start=33000,\n",
    "            mode='test')\n",
    "    else:\n",
    "        # Add more datasets\n",
    "        test_dataset = UIEBDataset(data_path, \n",
    "            label_path,\n",
    "            size=10,\n",
    "            test_start=500,\n",
    "            mode='test')\n",
    "\n",
    "    batch_size = 1\n",
    "    dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    _,_, mse_scores = test(fE, fI, dataloader, name, which_epoch)\n",
    "\n",
    "    print (\"Average MSE: {}\".format(sum(mse_scores)/len(mse_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "adfcd844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found uw images: 890\n",
      "Found cl images: 890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:12, 12.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw_img shape: torch.Size([3, 333, 500])\n",
      "fI_out shape: torch.Size([3, 333, 500])\n",
      "cl_img shape: torch.Size([3, 333, 500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:24, 12.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw_img shape: torch.Size([3, 333, 500])\n",
      "fI_out shape: torch.Size([3, 333, 500])\n",
      "cl_img shape: torch.Size([3, 333, 500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:45, 16.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw_img shape: torch.Size([3, 480, 640])\n",
      "fI_out shape: torch.Size([3, 480, 640])\n",
      "cl_img shape: torch.Size([3, 480, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:57, 14.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw_img shape: torch.Size([3, 333, 500])\n",
      "fI_out shape: torch.Size([3, 333, 500])\n",
      "cl_img shape: torch.Size([3, 333, 500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [01:08, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw_img shape: torch.Size([3, 333, 500])\n",
      "fI_out shape: torch.Size([3, 333, 500])\n",
      "cl_img shape: torch.Size([3, 333, 500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [01:28, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw_img shape: torch.Size([3, 480, 640])\n",
      "fI_out shape: torch.Size([3, 480, 640])\n",
      "cl_img shape: torch.Size([3, 480, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [01:40, 14.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw_img shape: torch.Size([3, 333, 500])\n",
      "fI_out shape: torch.Size([3, 333, 500])\n",
      "cl_img shape: torch.Size([3, 333, 500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [01:51, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw_img shape: torch.Size([3, 333, 500])\n",
      "fI_out shape: torch.Size([3, 333, 500])\n",
      "cl_img shape: torch.Size([3, 333, 500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [02:03, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw_img shape: torch.Size([3, 333, 500])\n",
      "fI_out shape: torch.Size([3, 333, 500])\n",
      "cl_img shape: torch.Size([3, 333, 500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:16, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw_img shape: torch.Size([3, 333, 500])\n",
      "fI_out shape: torch.Size([3, 333, 500])\n",
      "cl_img shape: torch.Size([3, 333, 500])\n",
      "Average MSE: 0.048664132878184316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "name = \"testdavid\"\n",
    "num_channels = 3\n",
    "which_epoch = False\n",
    "fe_load_path = \"C:/Users/davfa/Downloads/fE_86.pth\"\n",
    "fi_load_path = \"C:/Users/davfa/Downloads/fI_86.pth\"\n",
    "\n",
    "data_path = \"C:/Users/davfa/Downloads/raw-890/raw-890\"\n",
    "label_path = \"C:/Users/davfa/Downloads/reference-890/reference-890\"\n",
    "test_size = 10\n",
    "test_dataset = 'uieb'\n",
    "\n",
    "main(name, num_channels, test_dataset, data_path, label_path, which_epoch, test_size, fe_load_path, fi_load_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
